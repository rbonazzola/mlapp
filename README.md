# ML API

This code contains an implementation of the same app as in ml_api but following a microservices approach. It features a container for the API (at [ml_api](ml_api)), another for the model (at [model_service](model_service)), a redis container for communication and a postgres container for storing the predictions. They can be run with the [`docker compose`](ml_api_microservices/docker-compose.yml) file. It is the code corresponding to the examples shown in "Deploy microservice architecture locally".

It can be run with `docker compose up`.

## API endpoints:

- **[GET] /**: Hello world of the api
- **[POST] /predict**: Expects a json object containing a sample as described in (ml_api/routes/model_routes.py)[ml_api/routes/model_routes.py] and returns the prediction of the model
- **[GET] /predictions**: Returns the list of predictions performed by the model.

- In order to access the front-end and fill out a form to make a prediction, go to `localhost:9000`.

Also, **/docs** contains the automatic documentation of the API generated by FastAPI.
